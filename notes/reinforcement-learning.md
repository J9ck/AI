# ğŸ® Reinforcement Learning

## Table of Contents
- [Introduction](#introduction)
- [Core Concepts](#core-concepts)
- [Markov Decision Processes](#markov-decision-processes)
- [Value-Based Methods](#value-based-methods)
- [Policy-Based Methods](#policy-based-methods)
- [Actor-Critic Methods](#actor-critic-methods)
- [Deep Reinforcement Learning](#deep-reinforcement-learning)
- [Applications](#applications)

---

## Introduction

Reinforcement Learning (RL) is learning through interaction with an environment to maximize cumulative reward.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RL PARADIGM                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚              â”‚            ENVIRONMENT                   â”‚               â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                             â”‚                                           â”‚
â”‚                    state s, reward r                                    â”‚
â”‚                             â”‚                                           â”‚
â”‚                             â–¼                                           â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚              â”‚              AGENT                       â”‚               â”‚
â”‚              â”‚                                          â”‚               â”‚
â”‚              â”‚  Observes state â†’ Selects action         â”‚               â”‚
â”‚              â”‚  Receives reward â†’ Updates policy        â”‚               â”‚
â”‚              â”‚                                          â”‚               â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                             â”‚                                           â”‚
â”‚                         action a                                        â”‚
â”‚                             â”‚                                           â”‚
â”‚                             â–¼                                           â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚              â”‚            ENVIRONMENT                   â”‚               â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                         â”‚
â”‚   Goal: Learn policy Ï€ that maximizes expected cumulative reward       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RL vs Other Learning Paradigms

| Paradigm | Supervision | Feedback | Examples |
|----------|-------------|----------|----------|
| **Supervised** | Full labels | Immediate, exact | Classification |
| **Unsupervised** | No labels | None | Clustering |
| **Reinforcement** | Reward signal | Delayed, scalar | Game playing |

---

## Core Concepts

### Key Terminology

| Term | Symbol | Description |
|------|--------|-------------|
| **State** | $s$ | Current situation of the environment |
| **Action** | $a$ | Choice made by the agent |
| **Reward** | $r$ | Immediate feedback signal |
| **Policy** | $\pi(a\|s)$ | Strategy mapping states to actions |
| **Value** | $V(s)$ | Expected cumulative reward from state |
| **Q-Value** | $Q(s,a)$ | Expected cumulative reward from state-action pair |
| **Return** | $G_t$ | Total discounted reward from time $t$ |

### The RL Loop

```
t=0        t=1        t=2        t=3
 â”‚          â”‚          â”‚          â”‚
 sâ‚€ â”€â”€aâ‚€â”€â”€â–º sâ‚ â”€â”€aâ‚â”€â”€â–º sâ‚‚ â”€â”€aâ‚‚â”€â”€â–º sâ‚ƒ â”€â”€â–º ...
 â”‚    â”‚     â”‚    â”‚     â”‚    â”‚     â”‚
 â””â”€â”€râ‚â”€â”€â”˜   â””â”€â”€râ‚‚â”€â”€â”˜   â””â”€â”€râ‚ƒâ”€â”€â”˜   ...

Return: Gâ‚€ = râ‚ + Î³râ‚‚ + Î³Â²râ‚ƒ + ...
        where Î³ âˆˆ [0, 1] is discount factor
```

### Exploration vs Exploitation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXPLORATION-EXPLOITATION TRADEOFF                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   EXPLOITATION                    EXPLORATION                          â”‚
â”‚   "Use what you know"            "Try new things"                      â”‚
â”‚                                                                         â”‚
â”‚   Go to your favorite            Try a new restaurant                  â”‚
â”‚   restaurant                                                            â”‚
â”‚                                                                         â”‚
â”‚   Guaranteed good meal           Might find something better           â”‚
â”‚   but no improvement             but might be disappointing            â”‚
â”‚                                                                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚                                                                         â”‚
â”‚   Common strategies:                                                   â”‚
â”‚   â€¢ Îµ-greedy: Random action with probability Îµ                         â”‚
â”‚   â€¢ Softmax: Sample from action distribution                           â”‚
â”‚   â€¢ UCB: Upper Confidence Bound                                        â”‚
â”‚   â€¢ Thompson Sampling: Sample from posterior                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Markov Decision Processes

### Definition

An MDP is defined by tuple $(S, A, P, R, \gamma)$:
- $S$: Set of states
- $A$: Set of actions
- $P(s'|s, a)$: Transition probability
- $R(s, a, s')$: Reward function
- $\gamma$: Discount factor

### Markov Property

$$P(s_{t+1} | s_t, a_t, s_{t-1}, ..., s_0) = P(s_{t+1} | s_t, a_t)$$

"The future depends only on the present, not the past"

### Bellman Equations

**Value Function:**
$$V^\pi(s) = \mathbb{E}_\pi\left[\sum_{t=0}^{\infty} \gamma^t r_t | s_0 = s\right]$$

**Bellman Expectation Equation:**
$$V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^\pi(s')]$$

**Bellman Optimality Equation:**
$$V^*(s) = \max_a \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^*(s')]$$

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BELLMAN BACKUP DIAGRAM                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚                         V(s)                                            â”‚
â”‚                          â”‚                                              â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚          â”‚               â”‚               â”‚                             â”‚
â”‚          â–¼               â–¼               â–¼                             â”‚
â”‚         aâ‚              aâ‚‚              aâ‚ƒ        (policy chooses)     â”‚
â”‚          â”‚               â”‚               â”‚                             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”                        â”‚
â”‚     â”‚    â”‚    â”‚     â”‚    â”‚    â”‚     â”‚    â”‚    â”‚                        â”‚
â”‚     â–¼    â–¼    â–¼     â–¼    â–¼    â–¼     â–¼    â–¼    â–¼                        â”‚
â”‚    s'â‚  s'â‚‚  s'â‚ƒ   s'â‚  s'â‚‚  s'â‚ƒ   s'â‚  s'â‚‚  s'â‚ƒ  (transition probs)  â”‚
â”‚     â”‚    â”‚    â”‚     â”‚    â”‚    â”‚     â”‚    â”‚    â”‚                        â”‚
â”‚    V(s'â‚) ...                                       (recursive values) â”‚
â”‚                                                                         â”‚
â”‚   V(s) = Î£â‚ Ï€(a|s) Î£â‚›' P(s'|s,a) [R + Î³V(s')]                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Value-Based Methods

### Q-Learning

Learn action-value function directly (model-free, off-policy):

$$Q(s, a) \leftarrow Q(s, a) + \alpha[r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$$

```
Algorithm: Q-Learning
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Initialize Q(s, a) arbitrarily
For each episode:
    Initialize s
    For each step:
        Choose a from s using Îµ-greedy from Q
        Take action a, observe r, s'
        Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]
        s â† s'
    Until s is terminal
```

### SARSA

On-policy TD control:

$$Q(s, a) \leftarrow Q(s, a) + \alpha[r + \gamma Q(s', a') - Q(s, a)]$$

```
Q-Learning vs SARSA:

Q-Learning (off-policy):     SARSA (on-policy):
Update uses max Q(s',a')     Update uses actual a' taken
                             
Q(s,a) + Î±[r + Î³ max Q - Q]  Q(s,a) + Î±[r + Î³ Q(s',a') - Q]
              â†‘                              â†‘
         Best action                   Action actually taken
```

### TD Learning

**Temporal Difference**: Bootstrap from current estimates

$$V(s) \leftarrow V(s) + \alpha[\underbrace{r + \gamma V(s')}_{\text{TD target}} - V(s)]$$

```
                TD error = r + Î³V(s') - V(s)
                            â†‘
                    Bootstrapped estimate
                    (using current V)

Monte Carlo:     Wait until episode ends, use actual return
TD(0):           Use immediate reward + next state estimate
TD(Î»):           Blend of MC and TD with eligibility traces
```

---

## Policy-Based Methods

### Policy Gradient

Directly optimize the policy:

$$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$$

**Policy Gradient Theorem:**
$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(a|s) Q^{\pi_\theta}(s, a)\right]$$

### REINFORCE

```
Algorithm: REINFORCE (Monte Carlo Policy Gradient)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Initialize policy parameters Î¸
For each episode:
    Generate episode: sâ‚€, aâ‚€, râ‚, sâ‚, aâ‚, râ‚‚, ..., sâ‚œ
    For t = 0, 1, ..., T:
        G â† Î£_{k=t}^{T} Î³^{k-t} r_{k+1}    (return from step t)
        Î¸ â† Î¸ + Î± G âˆ‡_Î¸ log Ï€_Î¸(aâ‚œ|sâ‚œ)
```

### Advantages of Policy Gradient

| Aspect | Value-Based | Policy-Based |
|--------|-------------|--------------|
| Action space | Discrete (usually) | Continuous âœ“ |
| Stochastic policies | Hard | Natural âœ“ |
| Convergence | Can oscillate | Smoother âœ“ |
| Sample efficiency | Better | Lower |

---

## Actor-Critic Methods

Combine policy gradient (actor) with value function (critic):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ACTOR-CRITIC ARCHITECTURE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚                    Environment                                          â”‚
â”‚                        â”‚                                                â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚              â”‚  state s, reward r â”‚                                    â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                        â”‚                                                â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚          â”‚                           â”‚                                 â”‚
â”‚          â–¼                           â–¼                                 â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚    â”‚   ACTOR   â”‚              â”‚  CRITIC   â”‚                            â”‚
â”‚    â”‚   Ï€(a|s)  â”‚              â”‚   V(s)    â”‚                            â”‚
â”‚    â”‚           â”‚              â”‚           â”‚                            â”‚
â”‚    â”‚  Policy   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Value    â”‚                            â”‚
â”‚    â”‚  Network  â”‚  TD error    â”‚  Network  â”‚                            â”‚
â”‚    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚          â”‚                                                              â”‚
â”‚          â–¼                                                              â”‚
â”‚       action a                                                          â”‚
â”‚                                                                         â”‚
â”‚   Actor: Updates policy to maximize expected value                     â”‚
â”‚   Critic: Evaluates how good the actor's actions are                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Advantage Function

$$A(s, a) = Q(s, a) - V(s)$$

"How much better is action $a$ compared to average?"

### A2C/A3C

**Advantage Actor-Critic:**
$$\nabla_\theta J(\theta) = \mathbb{E}[\nabla_\theta \log \pi_\theta(a|s) A(s, a)]$$

**A3C**: Asynchronous variant with parallel actors

### PPO (Proximal Policy Optimization)

Most popular modern algorithm:

$$L^{CLIP}(\theta) = \mathbb{E}\left[\min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t)\right]$$

Where $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$

```
PPO Key Ideas:
1. Clipped objective prevents too large policy updates
2. Multiple epochs per batch (sample efficient)
3. Stable training without complex tuning
```

---

## Deep Reinforcement Learning

### DQN (Deep Q-Network)

Q-Learning + Deep Neural Networks:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DQN ARCHITECTURE                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   State (4 frames)                                                     â”‚
â”‚        â”‚                                                                â”‚
â”‚        â–¼                                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                          â”‚
â”‚   â”‚  Conv   â”‚  84Ã—84Ã—4 â†’ 32 filters                                    â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                                          â”‚
â”‚        â”‚                                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                                                          â”‚
â”‚   â”‚  Conv   â”‚  â†’ 64 filters                                            â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                                          â”‚
â”‚        â”‚                                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                                                          â”‚
â”‚   â”‚  Conv   â”‚  â†’ 64 filters                                            â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                                          â”‚
â”‚        â”‚                                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                                                          â”‚
â”‚   â”‚   FC    â”‚  512 units                                               â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                                          â”‚
â”‚        â”‚                                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                                                          â”‚
â”‚   â”‚   FC    â”‚  |A| outputs (Q-value per action)                        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                          â”‚
â”‚                                                                         â”‚
â”‚   Key innovations:                                                     â”‚
â”‚   â€¢ Experience Replay: Store and sample past transitions               â”‚
â”‚   â€¢ Target Network: Separate network for stable targets                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DQN Improvements

| Improvement | Description |
|-------------|-------------|
| **Double DQN** | Use online network to select, target to evaluate |
| **Dueling DQN** | Separate value and advantage streams |
| **Prioritized Replay** | Sample important transitions more often |
| **Noisy Nets** | Parametric exploration |
| **Rainbow** | Combines all improvements |

### Policy Gradient with Deep Networks

```
TRPO â†’ PPO â†’ SAC

TRPO: Trust Region Policy Optimization
      Complex constraint optimization

PPO: Proximal Policy Optimization  
     Simpler clipped objective
     
SAC: Soft Actor-Critic
     Maximum entropy RL
     Better exploration
```

---

## Applications

### Games

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RL IN GAMES                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   2013: DQN           Atari games (superhuman on many)                 â”‚
â”‚   2016: AlphaGo       Go (beat world champion)                         â”‚
â”‚   2017: AlphaZero     Chess, Shogi, Go (from scratch)                  â”‚
â”‚   2019: AlphaStar     StarCraft II (Grandmaster level)                 â”‚
â”‚   2019: OpenAI Five   Dota 2 (beat world champions)                    â”‚
â”‚   2022: Cicero        Diplomacy (human-level negotiation)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Real-World Applications

| Domain | Application |
|--------|-------------|
| **Robotics** | Robot control, manipulation |
| **Autonomous Vehicles** | Navigation, decision making |
| **Recommendation** | Personalized content |
| **Trading** | Portfolio optimization |
| **Healthcare** | Treatment optimization |
| **LLMs** | RLHF for alignment |

### RLHF for LLMs

```
Pre-training â†’ SFT â†’ Reward Model â†’ PPO Fine-tuning
                           â”‚
                           â””â”€â”€ Human preferences
                               "Which response is better?"
```

---

## Summary: Algorithm Selection

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WHEN TO USE WHAT                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Discrete Actions + Low Dimensional State:                            â”‚
â”‚   â†’ Q-Learning, DQN                                                    â”‚
â”‚                                                                         â”‚
â”‚   Continuous Actions:                                                  â”‚
â”‚   â†’ PPO, SAC, DDPG                                                     â”‚
â”‚                                                                         â”‚
â”‚   Simple Environment + Fast Iteration:                                 â”‚
â”‚   â†’ PPO (reliable, easy to tune)                                       â”‚
â”‚                                                                         â”‚
â”‚   Sample Efficiency Critical:                                          â”‚
â”‚   â†’ SAC, model-based methods                                           â”‚
â”‚                                                                         â”‚
â”‚   Multi-Agent:                                                         â”‚
â”‚   â†’ MARL algorithms (MAPPO, QMIX)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Resources

- ğŸ“š **Book**: "Reinforcement Learning" by Sutton & Barto (free online)
- ğŸ“ **Course**: David Silver's RL Course (DeepMind)
- ğŸ“ **Course**: UC Berkeley CS285 - Deep RL
- ğŸ“„ **Paper**: "Playing Atari with Deep RL" (DQN)
- ğŸ“„ **Paper**: "Proximal Policy Optimization Algorithms" (PPO)

---

ğŸŒ [Back to Notes](README.md) | ğŸ”— [Visit jgcks.com](https://www.jgcks.com)

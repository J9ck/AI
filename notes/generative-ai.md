# ğŸ¨ Generative AI

## Table of Contents
- [Introduction](#introduction)
- [Generative vs Discriminative](#generative-vs-discriminative)
- [Variational Autoencoders (VAEs)](#variational-autoencoders-vaes)
- [Generative Adversarial Networks (GANs)](#generative-adversarial-networks-gans)
- [Diffusion Models](#diffusion-models)
- [Large Language Models (LLMs)](#large-language-models-llms)
- [RLHF](#reinforcement-learning-from-human-feedback)
- [Practical Applications](#practical-applications)

---

## Introduction

Generative AI creates new contentâ€”text, images, audio, videoâ€”by learning patterns from existing data.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GENERATIVE AI LANDSCAPE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚                         GENERATIVE AI                                   â”‚
â”‚                              â”‚                                          â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚     â”‚            â”‚           â”‚           â”‚            â”‚                â”‚
â”‚     â–¼            â–¼           â–¼           â–¼            â–¼                â”‚
â”‚   TEXT        IMAGES       AUDIO       VIDEO        CODE               â”‚
â”‚     â”‚            â”‚           â”‚           â”‚            â”‚                â”‚
â”‚   LLMs       Diffusion    Speech      Video        Code                â”‚
â”‚   GPT-4      DALL-E 3     Synthesis   Generation   Assistants         â”‚
â”‚   Claude     Midjourney   Music       Sora         Copilot            â”‚
â”‚   LLaMA      Stable Diff  ElevenLabs               Cursor             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Generative vs Discriminative

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODEL COMPARISON                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   DISCRIMINATIVE                    GENERATIVE                         â”‚
â”‚   Learn: P(Y|X)                     Learn: P(X) or P(X|Y)              â”‚
â”‚                                                                         â”‚
â”‚   "Is this a cat or dog?"           "Generate a cat image"             â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚   â”‚             â”‚                   â”‚             â”‚                    â”‚
â”‚   â”‚     ğŸ±      â”‚ â†’ Model â†’ "Cat"   â”‚    Noise    â”‚ â†’ Model â†’ ğŸ±       â”‚
â”‚   â”‚             â”‚                   â”‚      z      â”‚                    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                         â”‚
â”‚   Examples:                         Examples:                          â”‚
â”‚   - Logistic Regression             - GANs                             â”‚
â”‚   - SVM                             - VAEs                             â”‚
â”‚   - Neural Classifiers              - Diffusion Models                 â”‚
â”‚                                     - LLMs                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Variational Autoencoders (VAEs)

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VAE ARCHITECTURE                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚        ENCODER                                 DECODER                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚                 â”‚                     â”‚                 â”‚          â”‚
â”‚   â”‚      Input      â”‚                     â”‚    Generated    â”‚          â”‚
â”‚   â”‚        x        â”‚                     â”‚        xÌ‚        â”‚          â”‚
â”‚   â”‚                 â”‚                     â”‚                 â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚            â”‚                                       â”‚                    â”‚
â”‚            â–¼                                       â”‚                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚  Neural Net    â”‚                     â”‚   Neural Net    â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚            â”‚                                       â”‚                    â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                â”‚                    â”‚
â”‚      â”‚           â”‚                                â”‚                    â”‚
â”‚      â–¼           â–¼                                â”‚                    â”‚
â”‚    â”Œâ”€â”€â”€â”       â”Œâ”€â”€â”€â”      REPARAMETERIZATION      â”‚                    â”‚
â”‚    â”‚ Î¼ â”‚       â”‚ Ïƒ â”‚            TRICK             â”‚                    â”‚
â”‚    â””â”€â”¬â”€â”˜       â””â”€â”¬â”€â”˜                              â”‚                    â”‚
â”‚      â”‚           â”‚        z = Î¼ + Ïƒ Ã— Îµ           â”‚                    â”‚
â”‚      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        (Îµ ~ N(0,1))            â”‚                    â”‚
â”‚            â”‚                    â”‚                 â”‚                    â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                           z (latent)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Loss Function

$$\mathcal{L} = \underbrace{\mathbb{E}_{q(z|x)}[\log p(x|z)]}_{\text{Reconstruction}} - \underbrace{D_{KL}(q(z|x) || p(z))}_{\text{Regularization}}$$

- **Reconstruction Loss**: How well can we reconstruct x from z?
- **KL Divergence**: Keep latent distribution close to prior N(0,1)

### Properties

- âœ… Smooth latent space (interpolation possible)
- âœ… Principled probabilistic framework
- âŒ Blurry outputs compared to GANs
- âŒ Limited sample quality

---

## Generative Adversarial Networks (GANs)

### The Game

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GAN FRAMEWORK                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                    â”‚    DISCRIMINATOR    â”‚                             â”‚
â”‚                    â”‚     (The Critic)    â”‚                             â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                               â”‚                                         â”‚
â”‚           "Real or Fake?"     â”‚     "Real or Fake?"                    â”‚
â”‚                 â–²             â”‚              â–²                          â”‚
â”‚                 â”‚             â”‚              â”‚                          â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”       â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                    â”‚
â”‚        â”‚ Real Images â”‚       â”‚       â”‚ Fake Imagesâ”‚                    â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚       â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                              â”‚              â”‚                          â”‚
â”‚                              â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                    â”‚
â”‚                              â”‚       â”‚  GENERATOR â”‚                    â”‚
â”‚                              â”‚       â”‚ (The Forger)â”‚                   â”‚
â”‚                              â”‚       â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                              â”‚              â”‚                          â”‚
â”‚                              â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                    â”‚
â”‚                              â”‚       â”‚   Noise    â”‚                    â”‚
â”‚                              â”‚       â”‚     z      â”‚                    â”‚
â”‚                              â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                              â”‚                                          â”‚
â”‚                    Generator tries to fool Discriminator               â”‚
â”‚                    Discriminator tries to catch fakes                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Training Objective

**Minimax Game:**
$$\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]$$

**Discriminator**: Maximize ability to distinguish real from fake
**Generator**: Minimize discriminator's ability to catch fakes

### GAN Variants

| Variant | Innovation | Application |
|---------|------------|-------------|
| **DCGAN** | Convolutional architecture | Image generation |
| **WGAN** | Wasserstein distance | Stable training |
| **StyleGAN** | Style-based generator | High-quality faces |
| **CycleGAN** | Unpaired translation | Style transfer |
| **Pix2Pix** | Paired image-to-image | Supervised translation |
| **BigGAN** | Large scale, class conditional | ImageNet generation |

### Mode Collapse

```
       Ideal Output                Mode Collapse
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  ğŸ± ğŸ• ğŸ¦ ğŸ  ğŸ´      â”‚    â”‚  ğŸ± ğŸ± ğŸ± ğŸ± ğŸ±      â”‚
   â”‚  Diverse samples    â”‚    â”‚  Same output        â”‚
   â”‚  from all modes     â”‚    â”‚  repeatedly         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Diffusion Models

### Forward Process (Add Noise)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DIFFUSION PROCESS                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   FORWARD PROCESS (Training): Add noise step by step                   â”‚
â”‚                                                                         â”‚
â”‚   xâ‚€ â”€â”€â”€â”€â”€â”€â–º xâ‚ â”€â”€â”€â”€â”€â”€â–º xâ‚‚ â”€â”€â”€â”€â”€â”€â–º ... â”€â”€â”€â”€â”€â”€â–º x_T                    â”‚
â”‚   ğŸ–¼ï¸         ğŸ–¼ï¸+noise   more noise              Pure noise             â”‚
â”‚                                                                         â”‚
â”‚   q(xâ‚œ|xâ‚œâ‚‹â‚) = N(xâ‚œ; âˆš(1-Î²â‚œ)xâ‚œâ‚‹â‚, Î²â‚œI)                                â”‚
â”‚                                                                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚                                                                         â”‚
â”‚   REVERSE PROCESS (Generation): Remove noise step by step              â”‚
â”‚                                                                         â”‚
â”‚   x_T â”€â”€â”€â”€â”€â”€â–º x_{T-1} â”€â”€â”€â”€â”€â”€â–º x_{T-2} â”€â”€â”€â”€â”€â”€â–º ... â”€â”€â”€â”€â”€â”€â–º xâ‚€          â”‚
â”‚   Noise       Less noise      Less noise              ğŸ–¼ï¸              â”‚
â”‚                                                                         â”‚
â”‚   p_Î¸(xâ‚œâ‚‹â‚|xâ‚œ) = N(xâ‚œâ‚‹â‚; Î¼_Î¸(xâ‚œ,t), Î£_Î¸(xâ‚œ,t))                        â”‚
â”‚   (Learned by neural network)                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Training Objective

Simplified loss (predict the noise):
$$L = \mathbb{E}_{t, x_0, \epsilon}[||\epsilon - \epsilon_\theta(x_t, t)||^2]$$

### DDPM vs DDIM

| Aspect | DDPM | DDIM |
|--------|------|------|
| Steps | 1000 | ~50-100 |
| Stochastic | Yes | Deterministic |
| Speed | Slow | Fast |
| Quality | High | High |

### Key Models

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DIFFUSION MODEL EVOLUTION                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   2020: DDPM                                                           â”‚
â”‚         â””â”€â”€ 2021: DDIM, Guided Diffusion                               â”‚
â”‚                    â””â”€â”€ 2022: DALL-E 2, Stable Diffusion, Imagen        â”‚
â”‚                              â””â”€â”€ 2023: SDXL, Midjourney v5             â”‚
â”‚                                        â””â”€â”€ 2024: SORA, Flux, SD3       â”‚
â”‚                                                                         â”‚
â”‚   Key Components:                                                       â”‚
â”‚   â€¢ U-Net architecture                                                 â”‚
â”‚   â€¢ Text conditioning (CLIP)                                           â”‚
â”‚   â€¢ Latent space (VAE)                                                 â”‚
â”‚   â€¢ Classifier-free guidance                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Large Language Models (LLMs)

### Scaling Laws

```
Performance âˆ f(Compute, Data, Parameters)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM SCALING                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Model Size (Params)                                                  â”‚
â”‚                                                                         â”‚
â”‚   GPT-4      â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚ ~1T+ ?        â”‚
â”‚   PaLM       â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚ 540B                    â”‚
â”‚   GPT-3      â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚ 175B                                â”‚
â”‚   LLaMA 2    â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚ 70B                                       â”‚
â”‚   GPT-2      â”‚â–ˆâ”‚ 1.5B                                                 â”‚
â”‚   BERT       â”‚â–â”‚ 340M                                                  â”‚
â”‚                                                                         â”‚
â”‚   Emergent abilities appear at scale:                                  â”‚
â”‚   â€¢ Chain-of-thought reasoning                                         â”‚
â”‚   â€¢ In-context learning                                                â”‚
â”‚   â€¢ Code generation                                                    â”‚
â”‚   â€¢ Mathematical reasoning                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture Evolution

```
GPT Architecture (Decoder-only):

Input: "The cat sat on the"
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Token Embeddings    â”‚
â”‚  + Position Embed    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼ Ã— N layers
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Masked Self-Attn    â”‚
â”‚  + Add & Norm        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Feed Forward        â”‚
â”‚  + Add & Norm        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LM Head (Softmax)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
Output: "mat" (next token prediction)
```

### In-Context Learning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    IN-CONTEXT LEARNING                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   ZERO-SHOT:                                                           â”‚
â”‚   "Translate to French: Hello" â†’ "Bonjour"                             â”‚
â”‚                                                                         â”‚
â”‚   ONE-SHOT:                                                            â”‚
â”‚   "English: Hello â†’ French: Bonjour                                    â”‚
â”‚    English: Goodbye â†’ French: ?"  â†’ "Au revoir"                        â”‚
â”‚                                                                         â”‚
â”‚   FEW-SHOT:                                                            â”‚
â”‚   "English: Hello â†’ French: Bonjour                                    â”‚
â”‚    English: Goodbye â†’ French: Au revoir                                â”‚
â”‚    English: Thank you â†’ French: Merci                                  â”‚
â”‚    English: Good morning â†’ French: ?"  â†’ "Bonjour"                     â”‚
â”‚                                                                         â”‚
â”‚   No gradient updates - learning happens in the context!               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Reinforcement Learning from Human Feedback

### RLHF Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RLHF TRAINING PIPELINE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   STEP 1: Supervised Fine-tuning (SFT)                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  Base LLM + Human demonstrations â†’ SFT Model                    â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                    â”‚                                    â”‚
â”‚                                    â–¼                                    â”‚
â”‚   STEP 2: Reward Model Training                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  Generate responses â†’ Human ranks them â†’ Train Reward Model     â”‚  â”‚
â”‚   â”‚                                                                  â”‚  â”‚
â”‚   â”‚  Prompt: "Explain quantum computing"                            â”‚  â”‚
â”‚   â”‚  Response A: [Good explanation]     â† Human prefers             â”‚  â”‚
â”‚   â”‚  Response B: [Mediocre explanation]                             â”‚  â”‚
â”‚   â”‚  Response C: [Poor explanation]                                 â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                    â”‚                                    â”‚
â”‚                                    â–¼                                    â”‚
â”‚   STEP 3: RL Optimization (PPO)                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  SFT Model + Reward Model â†’ PPO Training â†’ RLHF Model           â”‚  â”‚
â”‚   â”‚                                                                  â”‚  â”‚
â”‚   â”‚  Objective: Maximize reward while staying close to SFT model    â”‚  â”‚
â”‚   â”‚  (KL penalty prevents reward hacking)                           â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Constitutional AI (Anthropic)

Alternative to human feedback:
1. Generate responses
2. Critique using AI (based on constitution)
3. Revise based on critique
4. Train on revised responses

---

## Practical Applications

### Prompt Engineering

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROMPTING TECHNIQUES                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Chain-of-Thought:                                                    â”‚
â”‚   "Let's think step by step..."                                        â”‚
â”‚                                                                         â”‚
â”‚   Few-Shot Examples:                                                   â”‚
â”‚   "Here are some examples: [ex1], [ex2], [ex3]. Now do: ..."          â”‚
â”‚                                                                         â”‚
â”‚   Role Prompting:                                                      â”‚
â”‚   "You are an expert data scientist. Please..."                        â”‚
â”‚                                                                         â”‚
â”‚   Output Formatting:                                                   â”‚
â”‚   "Respond in JSON format with fields: ..."                            â”‚
â”‚                                                                         â”‚
â”‚   Self-Consistency:                                                    â”‚
â”‚   Generate multiple responses, take majority vote                      â”‚
â”‚                                                                         â”‚
â”‚   ReAct:                                                               â”‚
â”‚   Thought â†’ Action â†’ Observation â†’ Thought â†’ ...                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RAG (Retrieval-Augmented Generation)

```
Query: "What is the capital of France?"
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Embed    â”‚
    â”‚  Query    â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Search   â”‚â”€â”€â”€â”€â–ºâ”‚ Vector Database â”‚
    â”‚  Similar  â”‚     â”‚ (Knowledge)     â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼ Retrieved context
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Augment  â”‚
    â”‚  Prompt   â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚ "Based on: [Paris is the capital...]"
          â”‚ "Answer: What is the capital of France?"
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    LLM    â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    "Paris is the capital of France."
```

---

## Resources

- ğŸ“„ **Paper**: "Generative Adversarial Networks" (Goodfellow et al., 2014)
- ğŸ“„ **Paper**: "Denoising Diffusion Probabilistic Models" (Ho et al., 2020)
- ğŸ“„ **Paper**: "Language Models are Few-Shot Learners" (GPT-3)
- ğŸ“„ **Paper**: "Training language models to follow instructions" (InstructGPT)
- ğŸ“ **Course**: Stanford CS236 - Deep Generative Models

---

ğŸŒ [Back to Notes](README.md) | ğŸ”— [Visit jgcks.com](https://www.jgcks.com)

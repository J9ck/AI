# ğŸ“– AI/ML Notes

Welcome to the notes section! This is where I document comprehensive theory and concepts in AI/ML.

## ğŸ“‘ Contents

| Topic | Description | File |
|-------|-------------|------|
| ğŸ¯ **Machine Learning Fundamentals** | Core ML concepts, algorithms, and evaluation | [machine-learning-fundamentals.md](machine-learning-fundamentals.md) |
| ğŸ§  **Deep Learning** | Neural networks, backpropagation, architectures | [deep-learning.md](deep-learning.md) |
| ğŸ”„ **Transformers** | Attention mechanism, BERT, GPT, and beyond | [transformers.md](transformers.md) |
| ğŸ—£ï¸ **NLP** | Text processing, embeddings, language models | [nlp.md](nlp.md) |
| ğŸ‘ï¸ **Computer Vision** | CNNs, object detection, segmentation | [computer-vision.md](computer-vision.md) |
| ğŸ¨ **Generative AI** | GANs, VAEs, Diffusion models, LLMs | [generative-ai.md](generative-ai.md) |
| ğŸ® **Reinforcement Learning** | RL fundamentals, Q-learning, policy gradient | [reinforcement-learning.md](reinforcement-learning.md) |
| âš™ï¸ **MLOps** | Deployment, monitoring, pipelines | [mlops.md](mlops.md) |

## ğŸ“ Learning Path

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     START YOUR AI JOURNEY HERE!     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Machine Learning Fundamentals    â”‚
                    â”‚    (Start here if you're new!)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Deep Learning     â”‚         â”‚ Reinforcement Learningâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      NLP      â”‚       â”‚Computer Visionâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Transformers      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    Generative AI      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        MLOps          â”‚
        â”‚   (When deploying)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“š How to Use These Notes

1. **Sequential Learning**: Follow the learning path above for a structured approach
2. **Reference**: Jump directly to topics you need to brush up on
3. **Deep Dive**: Each note contains links to papers and resources for further reading

---

ğŸŒ [Back to Main Repository](../README.md) | ğŸ”— [Visit jgcks.com](https://www.jgcks.com)
# ğŸ“š AI/ML Notes - Concepts & Theory

> A comprehensive guide to understanding Artificial Intelligence and Machine Learning fundamentals.

[â† Back to Main](../README.md)

---

## ğŸ“‹ Table of Contents

- [Machine Learning Fundamentals](#-machine-learning-fundamentals)
- [Neural Networks](#-neural-networks)
- [Deep Learning Architectures](#-deep-learning-architectures)
- [Natural Language Processing](#-natural-language-processing)
- [Computer Vision](#-computer-vision)
- [Generative AI](#-generative-ai)
- [Reinforcement Learning](#-reinforcement-learning)
- [AI Ethics & Safety](#ï¸-ai-ethics--safety)

---

## ğŸ§  Machine Learning Fundamentals

### Supervised vs Unsupervised Learning

| Aspect | Supervised Learning | Unsupervised Learning |
|--------|---------------------|----------------------|
| **Data** | Labeled data (input-output pairs) | Unlabeled data |
| **Goal** | Learn mapping from inputs to outputs | Discover hidden patterns/structure |
| **Examples** | Classification, Regression | Clustering, Dimensionality Reduction |
| **Algorithms** | Linear Regression, SVM, Random Forest | K-Means, PCA, DBSCAN |

### Types of Machine Learning Problems

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Machine Learning Tasks                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    Supervised      â”‚   Unsupervised     â”‚    Reinforcement      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Classification   â”‚ â€¢ Clustering       â”‚ â€¢ Policy Learning     â”‚
â”‚ â€¢ Regression       â”‚ â€¢ Dim. Reduction   â”‚ â€¢ Value Learning      â”‚
â”‚ â€¢ Ranking          â”‚ â€¢ Anomaly Detectionâ”‚ â€¢ Model-Based         â”‚
â”‚ â€¢ Forecasting      â”‚ â€¢ Association      â”‚ â€¢ Model-Free          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Regression

**Goal**: Predict continuous numerical values.

**Common Algorithms**:
- **Linear Regression**: Fits a linear relationship between features and target
  - Equation: `y = mx + b` (simple) or `y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™` (multiple)
- **Polynomial Regression**: Captures non-linear relationships
- **Ridge Regression (L2)**: Linear regression with L2 regularization to prevent overfitting
- **Lasso Regression (L1)**: Linear regression with L1 regularization (can zero out features)
- **Elastic Net**: Combination of L1 and L2 regularization

**Key Metrics**:
- Mean Squared Error (MSE): `Î£(y_pred - y_actual)Â² / n`
- Root Mean Squared Error (RMSE): `âˆšMSE`
- Mean Absolute Error (MAE): `Î£|y_pred - y_actual| / n`
- RÂ² Score (Coefficient of Determination): Measures variance explained

### Classification

**Goal**: Predict discrete class labels.

**Binary Classification** (2 classes):
- Logistic Regression
- Support Vector Machines (SVM)
- Decision Trees

**Multi-class Classification** (>2 classes):
- One-vs-Rest (OvR)
- One-vs-One (OvO)
- Softmax Regression

**Key Metrics**:
```
                    Predicted
                  Pos    Neg
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
Actual  Pos   â”‚  TP   â”‚  FN   â”‚
              â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
        Neg   â”‚  FP   â”‚  TN   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

Accuracy  = (TP + TN) / (TP + TN + FP + FN)
Precision = TP / (TP + FP)  -- "Of predicted positives, how many correct?"
Recall    = TP / (TP + FN)  -- "Of actual positives, how many found?"
F1 Score  = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

### Clustering

**Goal**: Group similar data points without labels.

**Algorithms**:
- **K-Means**: Partitions data into K clusters based on centroid distance
  - Choose K (number of clusters)
  - Initialize centroids randomly
  - Assign points to nearest centroid
  - Update centroids to cluster mean
  - Repeat until convergence
  
- **Hierarchical Clustering**: Creates tree-like structure of clusters
  - Agglomerative (bottom-up)
  - Divisive (top-down)
  
- **DBSCAN**: Density-based clustering, finds arbitrary shaped clusters
  - Core points: Points with many neighbors
  - Border points: Near core points
  - Noise points: Neither core nor border

### Feature Engineering

**The process of using domain knowledge to create features that make ML algorithms work better.**

**Techniques**:
| Technique | Description | Example |
|-----------|-------------|---------|
| **Normalization** | Scale features to [0, 1] | `(x - min) / (max - min)` |
| **Standardization** | Scale to mean=0, std=1 | `(x - Î¼) / Ïƒ` |
| **One-Hot Encoding** | Convert categorical to binary | `color: [1,0,0], [0,1,0], [0,0,1]` |
| **Binning** | Convert continuous to categorical | Age groups: 0-18, 19-35, 36-50 |
| **Log Transform** | Handle skewed distributions | `log(x + 1)` |
| **Polynomial Features** | Create interaction terms | `xâ‚, xâ‚‚, xâ‚Â², xâ‚‚Â², xâ‚xâ‚‚` |

### Train/Test Split & Cross-Validation

```
Dataset Split Strategy:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Full Dataset                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Training Set         â”‚    Val    â”‚   Test Set    â”‚
â”‚           (60-70%)          â”‚  (10-15%) â”‚   (20-25%)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

K-Fold Cross-Validation (K=5):
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ Val â”‚Trainâ”‚Trainâ”‚Trainâ”‚Trainâ”‚  Fold 1
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚Trainâ”‚ Val â”‚Trainâ”‚Trainâ”‚Trainâ”‚  Fold 2
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚Trainâ”‚Trainâ”‚ Val â”‚Trainâ”‚Trainâ”‚  Fold 3
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚Trainâ”‚Trainâ”‚Trainâ”‚ Val â”‚Trainâ”‚  Fold 4
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚Trainâ”‚Trainâ”‚Trainâ”‚Trainâ”‚ Val â”‚  Fold 5
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
Final score = Average of all fold scores
```

### Bias-Variance Tradeoff

```
Total Error = BiasÂ² + Variance + Irreducible Error

High Bias (Underfitting):
- Model too simple
- Can't capture patterns
- High training error
- High test error

High Variance (Overfitting):
- Model too complex
- Memorizes training data
- Low training error
- High test error

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Model Complexity vs Error                       â”‚
â”‚                                                         â”‚
â”‚  Error                                                  â”‚
â”‚    â”‚                                                    â”‚
â”‚    â”‚   â•²                                   â•±           â”‚
â”‚    â”‚    â•²   Total Error                   â•±            â”‚
â”‚    â”‚     â•²                               â•±             â”‚
â”‚    â”‚      â•²â”€â”€â”€â”€â”€â•²                 â•±â”€â”€â”€â”€â”€â•±              â”‚
â”‚    â”‚       Bias  â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±  Variance          â”‚
â”‚    â”‚              â•²             â•±                      â”‚
â”‚    â”‚               â•²___________â•±                       â”‚
â”‚    â”‚                    â”‚                              â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚                   Optimal Complexity                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Solutions**:
- High Bias: More features, more complex model, less regularization
- High Variance: More data, simpler model, more regularization, dropout

---

## ğŸ”® Neural Networks

### Perceptron - The Building Block

```
        xâ‚ â”€â”€â†’ wâ‚ â”€â”€â”
                    â”‚
        xâ‚‚ â”€â”€â†’ wâ‚‚ â”€â”€â”¼â”€â”€â†’ Î£ â”€â”€â†’ f(z) â”€â”€â†’ output
                    â”‚
        xâ‚ƒ â”€â”€â†’ wâ‚ƒ â”€â”€â”˜
                    â†‘
                   bias (b)

z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + wâ‚ƒxâ‚ƒ + b
output = f(z)  where f is activation function
```

### Activation Functions

| Function | Formula | Range | Use Case |
|----------|---------|-------|----------|
| **Sigmoid** | `Ïƒ(x) = 1 / (1 + eâ»Ë£)` | (0, 1) | Binary classification output |
| **Tanh** | `tanh(x) = (eË£ - eâ»Ë£) / (eË£ + eâ»Ë£)` | (-1, 1) | Hidden layers (centered output) |
| **ReLU** | `max(0, x)` | [0, âˆ) | Hidden layers (default choice) |
| **Leaky ReLU** | `max(0.01x, x)` | (-âˆ, âˆ) | Prevents dying ReLU |
| **Softmax** | `eË£â± / Î£eË£Ê²` | (0, 1) | Multi-class classification output |
| **GELU** | `x Â· Î¦(x)` | (-âˆ, âˆ) | Transformers |

```
ReLU Graph:           Sigmoid Graph:        Tanh Graph:
    â”‚    â•±                â”‚    ____             â”‚    ____
    â”‚   â•±                 â”‚   â•±                 â”‚   â•±
â”€â”€â”€â”€â”¼â”€â”€â•±â”€â”€â”€â”€          â”€â”€â”€â”€â”¼â”€â”€â•±â”€â”€â”€â”€          â”€â”€â”€â”€â”¼â”€â•±â”€â”€â”€â”€
    â”‚â•±                    â”‚â•±                   â•±â”‚
    â”‚                     â”‚                   â•± â”‚
                                         â”€â”€â”€â”€   â”‚
```

### Backpropagation

**The algorithm used to train neural networks by computing gradients.**

**Steps**:
1. **Forward Pass**: Compute predictions layer by layer
2. **Compute Loss**: Compare predictions with actual values
3. **Backward Pass**: Compute gradients using chain rule
4. **Update Weights**: Adjust weights using gradients

```
Chain Rule Application:
âˆ‚Loss/âˆ‚wâ‚ = âˆ‚Loss/âˆ‚output Ã— âˆ‚output/âˆ‚z Ã— âˆ‚z/âˆ‚wâ‚
```

### Gradient Descent Variants

| Variant | Description | Batch Size |
|---------|-------------|------------|
| **Batch GD** | Use all samples per update | Full dataset |
| **Stochastic GD** | Use one sample per update | 1 |
| **Mini-batch GD** | Use subset per update | 16-256 typically |

**Update Rule**:
```
w = w - Î· Ã— âˆ‡L(w)

where:
Î· = learning rate
âˆ‡L(w) = gradient of loss with respect to weights
```

### Learning Rate

```
Learning Rate Effects:
                                              
Too Small:                Too Large:           Just Right:
â”‚                         â”‚                    â”‚
â”‚  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚    â•±â•²     â•±â•²      â”‚     â•²
â”‚ â•±                       â”‚   â•±  â•²   â•±  â•²     â”‚      â•²___
â”‚â•±                        â”‚  â•±    â•²_â•±    â•²    â”‚         â•²__
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚ â•±             â•²   â”‚            â•²_
(Very slow convergence)   (Divergence)        (Good convergence)
```

**Learning Rate Schedules**:
- **Step Decay**: Reduce LR by factor every N epochs
- **Exponential Decay**: `lr = lrâ‚€ Ã— e^(-kt)`
- **Cosine Annealing**: Smoothly decrease and optionally restart
- **Warmup**: Start small, gradually increase, then decay

### Optimizers

| Optimizer | Key Idea | Update Rule |
|-----------|----------|-------------|
| **SGD** | Basic gradient descent | `w -= lr Ã— g` |
| **Momentum** | Accumulate velocity | `v = Î²v + g; w -= lr Ã— v` |
| **RMSprop** | Adaptive learning rates | Scales by running avg of squared gradients |
| **Adam** | Momentum + RMSprop | Combines both approaches |
| **AdamW** | Adam + Weight Decay | Decoupled weight decay regularization |

**Adam** (Most commonly used):
```
m = Î²â‚ Ã— m + (1 - Î²â‚) Ã— g          # First moment (momentum)
v = Î²â‚‚ Ã— v + (1 - Î²â‚‚) Ã— gÂ²         # Second moment (RMSprop)
mÌ‚ = m / (1 - Î²â‚áµ—)                  # Bias correction
vÌ‚ = v / (1 - Î²â‚‚áµ—)
w = w - lr Ã— mÌ‚ / (âˆšvÌ‚ + Îµ)

Common values: Î²â‚=0.9, Î²â‚‚=0.999, Îµ=1e-8
```

---

## ğŸ—ï¸ Deep Learning Architectures

### Convolutional Neural Networks (CNNs)

**Key Operations**:

```
Convolution Operation:

Input:                    Kernel (3Ã—3):           Output:
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”    â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”    
â”‚ 1 â”‚ 2 â”‚ 3 â”‚ 0 â”‚ 1 â”‚    â”‚ 1 â”‚ 0 â”‚ 1 â”‚          â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤    â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤          â”‚12 â”‚...â”‚...â”‚
â”‚ 0 â”‚ 1 â”‚ 2 â”‚ 3 â”‚ 0 â”‚ * â”‚ 0 â”‚ 1 â”‚ 0 â”‚    =     â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤    â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤          â”‚...â”‚...â”‚...â”‚
â”‚ 1 â”‚ 0 â”‚ 1 â”‚ 0 â”‚ 2 â”‚    â”‚ 1 â”‚ 0 â”‚ 1 â”‚          â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤    â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
â”‚ 2 â”‚ 1 â”‚ 0 â”‚ 1 â”‚ 1 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚ 0 â”‚ 0 â”‚ 1 â”‚ 2 â”‚ 0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

Pooling (Max Pool 2Ã—2):

Input:              Output:
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”   â”Œâ”€â”€â”€â”¬â”€â”€â”€â”
â”‚ 1 â”‚ 3 â”‚ 2 â”‚ 1 â”‚   â”‚ 4 â”‚ 2 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤   â”œâ”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚ 4 â”‚ 2 â”‚ 1 â”‚ 0 â”‚ â†’ â”‚ 5 â”‚ 3 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤   â””â”€â”€â”€â”´â”€â”€â”€â”˜
â”‚ 1 â”‚ 5 â”‚ 3 â”‚ 2 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚ 2 â”‚ 1 â”‚ 0 â”‚ 1 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
```

**CNN Terms**:
- **Stride**: Step size of the kernel movement
- **Padding**: Adding zeros around input to control output size
  - `VALID`: No padding (output shrinks)
  - `SAME`: Pad to keep output same size as input
- **Receptive Field**: Input region that affects a particular output

**Output Size Formula**:
```
Output = (Input - Kernel + 2Ã—Padding) / Stride + 1
```

### Recurrent Neural Networks (RNNs)

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                         â”‚
                    â†“                                         â”‚
Input: xâ‚ â”€â”€â†’ [RNN Cell] â”€â”€â†’ hâ‚ â”€â”€â†’ yâ‚                       â”‚
                    â”‚                                         â”‚
                    â†“                                         â”‚
Input: xâ‚‚ â”€â”€â†’ [RNN Cell] â”€â”€â†’ hâ‚‚ â”€â”€â†’ yâ‚‚                       â”‚
                    â”‚                                         â”‚
                    â†“                                         â”‚
Input: xâ‚ƒ â”€â”€â†’ [RNN Cell] â”€â”€â†’ hâ‚ƒ â”€â”€â†’ yâ‚ƒ                       â”‚
                                                              â”‚
                                      Hidden state loops â”€â”€â”€â”€â”€â”˜

h_t = tanh(W_hh Ã— h_{t-1} + W_xh Ã— x_t + b)
```

**Problem**: Vanishing/Exploding Gradients over long sequences

### LSTMs (Long Short-Term Memory)

**Solves RNN's vanishing gradient problem using gates.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LSTM Cell                             â”‚
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚ Forget   â”‚  â”‚  Input   â”‚  â”‚  Output  â”‚                  â”‚
â”‚   â”‚  Gate    â”‚  â”‚  Gate    â”‚  â”‚  Gate    â”‚                  â”‚
â”‚   â”‚  (f_t)   â”‚  â”‚  (i_t)   â”‚  â”‚  (o_t)   â”‚                  â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚        â”‚             â”‚             â”‚                         â”‚
â”‚        â†“             â†“             â†“                         â”‚
â”‚   c_{t-1} â”€â”€â†’ [Ã—] â”€â”€[+]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ c_t (cell state)   â”‚
â”‚                â†‘     â†‘                                       â”‚
â”‚                f_t   i_t Ã— cÌƒ_t                               â”‚
â”‚                                                              â”‚
â”‚   h_t = o_t Ã— tanh(c_t)                                     â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Gates:
- Forget Gate: What to remove from cell state
- Input Gate: What new info to add
- Output Gate: What to output from cell state
```

### GRUs (Gated Recurrent Units)

**Simplified version of LSTM with fewer gates.**

```
GRU has 2 gates instead of 3:
- Reset Gate (r_t): Controls how much past info to forget
- Update Gate (z_t): Controls how much past info to keep

Fewer parameters = faster training
Often performs comparably to LSTM
```

### Transformers

**The architecture powering modern AI (BERT, GPT, etc.)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Transformer Architecture                  â”‚
â”‚                                                              â”‚
â”‚    Input                                    Output           â”‚
â”‚      â†“                                        â†‘              â”‚
â”‚  [Embedding]                           [Linear + Softmax]    â”‚
â”‚      â†“                                        â†‘              â”‚
â”‚  [Positional                           [Feed Forward]        â”‚
â”‚   Encoding]                                   â†‘              â”‚
â”‚      â†“                                 [Add & Norm]          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â†‘              â”‚
â”‚  â”‚ Encoder â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [Multi-Head           â”‚
â”‚  â”‚  Ã—N     â”‚                           Cross-Attention]     â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                  â†‘              â”‚
â”‚       â”‚                                [Add & Norm]          â”‚
â”‚       â”‚                                       â†‘              â”‚
â”‚       â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [Masked Multi-Head       â”‚
â”‚       â”‚     â”‚                       Self-Attention]          â”‚
â”‚       â”‚     â†“                                â†‘              â”‚
â”‚       â””â”€â”€â”€â”€â†’ Decoder Ã—N â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Attention Mechanism

**"Attention is All You Need" - The core innovation**

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V

Q = Query (what we're looking for)
K = Key (what we have to match against)
V = Value (what we return)
d_k = dimension of keys (for scaling)

Multi-Head Attention:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚ Head 1 â”‚  â”‚ Head 2 â”‚  â”‚ Head 3 â”‚  ...   â”‚ Head h â”‚     â”‚
â”‚   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â”‚
â”‚       â”‚           â”‚           â”‚                  â”‚          â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                          â”‚                                   â”‚
â”‚                    [Concatenate]                            â”‚
â”‚                          â”‚                                   â”‚
â”‚                    [Linear Layer]                           â”‚
â”‚                          â”‚                                   â”‚
â”‚                       Output                                â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Multiple heads allow model to attend to different aspects simultaneously
```

### Self-Attention

**Each position attends to all positions in the same sequence.**

```
Input: "The cat sat on the mat"

       The   cat   sat   on   the   mat
The    0.1   0.3   0.2   0.1  0.1   0.2   â† attention weights
cat    0.1   0.4   0.2   0.1  0.1   0.1      for each word
sat    0.1   0.2   0.3   0.2  0.1   0.1
...

Each word "attends" to every other word
Learns which words are relevant to each other
```

---

## ğŸ’¬ Natural Language Processing

### Tokenization

**Breaking text into smaller units (tokens).**

| Type | Example | Output |
|------|---------|--------|
| **Word** | "Hello world" | ["Hello", "world"] |
| **Character** | "Hello" | ["H", "e", "l", "l", "o"] |
| **Subword (BPE)** | "unhappiness" | ["un", "happiness"] |
| **WordPiece** | "playing" | ["play", "##ing"] |

```
BPE (Byte Pair Encoding):
1. Start with character-level vocabulary
2. Count all adjacent pairs
3. Merge most frequent pair
4. Repeat until vocabulary size reached

Handles:
- Out-of-vocabulary words
- Morphologically rich languages
- Reduces vocabulary size
```

### Word Embeddings

**Representing words as dense vectors.**

**Word2Vec**:
- **CBOW**: Predict center word from context
- **Skip-gram**: Predict context from center word

```
Example embeddings (simplified 3D):
king    = [0.8, 0.2, 0.9]
queen   = [0.7, 0.8, 0.9]
man     = [0.9, 0.1, 0.5]
woman   = [0.8, 0.7, 0.5]

king - man + woman â‰ˆ queen  (famous analogy)
```

**GloVe (Global Vectors)**:
- Uses word co-occurrence statistics
- Combines benefits of count-based and predictive methods

### BERT (Bidirectional Encoder Representations from Transformers)

```
Architecture:
- Encoder-only Transformer
- Bidirectional context (sees left AND right)
- Pre-trained on MLM and NSP tasks

Pre-training Tasks:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Masked Language Modeling (MLM):                             â”‚
â”‚ Input:  "The [MASK] sat on the mat"                        â”‚
â”‚ Output: "cat" (predict masked word)                         â”‚
â”‚                                                              â”‚
â”‚ Next Sentence Prediction (NSP):                             â”‚
â”‚ Input:  "[CLS] Sentence A [SEP] Sentence B [SEP]"          â”‚
â”‚ Output: Is B the actual next sentence? (Yes/No)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Models: BERT-base (110M params), BERT-large (340M params)
Variants: RoBERTa, ALBERT, DistilBERT, DeBERTa
```

### GPT Architecture (Generative Pre-trained Transformer)

```
Architecture:
- Decoder-only Transformer
- Autoregressive (left-to-right generation)
- Causal masking (can't see future tokens)

Generation Process:
Input:  "Once upon a"
Step 1: Predict "time" â†’ "Once upon a time"
Step 2: Predict "there" â†’ "Once upon a time there"
Step 3: Predict "was" â†’ "Once upon a time there was"
...

Evolution:
GPT-1:   117M parameters
GPT-2:   1.5B parameters
GPT-3:   175B parameters
GPT-4:   Estimated ~1.7T parameters (MoE)
```

### Sequence-to-Sequence Models

```
Encoder-Decoder Architecture:

Input: "Hello, how are you?"
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Encoder          â”‚
â”‚  (Processes input seq)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
     Context Vector
            â”‚
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Decoder          â”‚
â”‚ (Generates output seq)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
Output: "Bonjour, comment allez-vous?"

Applications:
- Machine Translation
- Text Summarization
- Question Answering
```

### Named Entity Recognition (NER)

**Identifying and classifying named entities in text.**

```
Input: "Apple Inc. was founded by Steve Jobs in California."

Output with BIO tagging:
Apple     B-ORG
Inc.      I-ORG
was       O
founded   O
by        O
Steve     B-PER
Jobs      I-PER
in        O
California B-LOC
.         O

Entity Types: PER (Person), ORG (Organization), LOC (Location), 
              DATE, TIME, MONEY, etc.
```

### Sentiment Analysis

```
Approaches:
1. Rule-based: Lexicon matching
2. ML-based: Train classifiers on labeled data
3. Deep Learning: Fine-tune BERT/RoBERTa

Example Classifications:
"I love this product!" â†’ Positive (0.95)
"Terrible experience." â†’ Negative (0.87)
"It's okay, I guess." â†’ Neutral (0.62)

Advanced: Aspect-based sentiment
"The food was great but the service was terrible."
â†’ Food: Positive, Service: Negative
```

---

## ğŸ‘ï¸ Computer Vision

### Image Classification

```
Input Image (224Ã—224Ã—3)
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Conv Layers     â”‚  â† Feature extraction
â”‚   (Hierarchical)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pooling Layers   â”‚  â† Reduce dimensions
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Flatten         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Fully Connected  â”‚  â† Classification
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
    Output: "Cat" (softmax over classes)
```

### Object Detection

**YOLO (You Only Look Once)**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ YOLO Approach:                                               â”‚
â”‚                                                              â”‚
â”‚ 1. Divide image into SÃ—S grid                               â”‚
â”‚ 2. Each cell predicts B bounding boxes                      â”‚
â”‚ 3. Each box: (x, y, w, h, confidence)                       â”‚
â”‚ 4. Each cell predicts class probabilities                   â”‚
â”‚                                                              â”‚
â”‚ Output: All predictions in single forward pass (FAST!)      â”‚
â”‚                                                              â”‚
â”‚ Versions: YOLOv1 â†’ v2 â†’ v3 â†’ v4 â†’ v5 â†’ v8                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**R-CNN Family**:
```
R-CNN:
Image â†’ Region Proposals â†’ CNN â†’ SVM Classification
(Slow: ~47s per image)

Fast R-CNN:
Image â†’ CNN â†’ Region Proposals â†’ Classification
(Faster: ~2s per image)

Faster R-CNN:
Image â†’ CNN â†’ Region Proposal Network â†’ Classification
(Even faster: ~0.2s per image)
```

### Image Segmentation

```
Types:

Semantic Segmentation:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Original      â”‚   â†’    â”‚ â–ˆâ–ˆâ–ˆ Person      â”‚
â”‚   Image         â”‚        â”‚ â–‘â–‘â–‘ Background  â”‚
â”‚                 â”‚        â”‚ â–“â–“â–“ Car         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
(Each pixel gets a class label)

Instance Segmentation:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Original      â”‚   â†’    â”‚ â–ˆâ–ˆâ–ˆ Person 1    â”‚
â”‚   Image         â”‚        â”‚ â–’â–’â–’ Person 2    â”‚
â”‚                 â”‚        â”‚ â–“â–“â–“ Car 1       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
(Each pixel gets class AND instance ID)

Architectures: U-Net, DeepLab, Mask R-CNN
```

### Data Augmentation

**Artificially expanding training data through transformations.**

| Augmentation | Description |
|--------------|-------------|
| **Horizontal Flip** | Mirror image left-right |
| **Rotation** | Rotate by random angle |
| **Scaling** | Zoom in/out |
| **Translation** | Shift image position |
| **Color Jitter** | Adjust brightness, contrast, saturation |
| **Random Crop** | Extract random regions |
| **Cutout** | Randomly mask square regions |
| **MixUp** | Blend two images and labels |
| **CutMix** | Replace image regions from another image |

### Transfer Learning

```
Strategy:
1. Take pre-trained model (e.g., ResNet trained on ImageNet)
2. Remove final classification layer
3. Add new layer for your task
4. Option A: Freeze base, train only new layers
   Option B: Fine-tune entire model with small learning rate

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pre-trained Model          â”‚ Your Model                     â”‚
â”‚                            â”‚                                â”‚
â”‚ [Conv Layers - FROZEN]     â”‚ [Conv Layers - FROZEN]        â”‚
â”‚         â†“                  â”‚         â†“                      â”‚
â”‚ [FC Layer]                 â”‚ [New FC Layer] â† Train this   â”‚
â”‚         â†“                  â”‚         â†“                      â”‚
â”‚ [1000 ImageNet classes]    â”‚ [Your 10 classes]             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
- Less data needed
- Faster training
- Often better performance
```

---

## âœ¨ Generative AI

### GANs (Generative Adversarial Networks)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GAN Architecture                         â”‚
â”‚                                                              â”‚
â”‚   Random        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      Generated              â”‚
â”‚   Noise  â”€â”€â”€â”€â”€â”€â†’â”‚  Generator  â”‚â”€â”€â”€â”€â”€â”€â†’  Image               â”‚
â”‚   (z)           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚                  â”‚
â”‚                                          â†“                  â”‚
â”‚                 Real          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚                 Images â”€â”€â”€â”€â”€â”€â†’â”‚  Discriminator  â”‚â”€â”€â†’ Real?  â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   Fake?   â”‚
â”‚                                                              â”‚
â”‚   Training:                                                  â”‚
â”‚   - Generator tries to fool Discriminator                   â”‚
â”‚   - Discriminator tries to distinguish real from fake       â”‚
â”‚   - Adversarial training until equilibrium                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Variants:
- DCGAN: Deep Convolutional GAN
- StyleGAN: Style-based generator
- CycleGAN: Image-to-image translation
- Pix2Pix: Paired image translation
```

### VAEs (Variational Autoencoders)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VAE Architecture                          â”‚
â”‚                                                              â”‚
â”‚   Input    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Î¼, Ïƒ     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Output    â”‚
â”‚   Image â”€â”€â†’â”‚ Encoder â”‚â”€â”€â†’ z â†â”€â”€â”€â”€â†’â”‚ Decoder â”‚â”€â”€â†’ Image    â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (latent)   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                              â”‚
â”‚   Key Idea: Learn latent distribution, not just encoding    â”‚
â”‚                                                              â”‚
â”‚   Loss = Reconstruction Loss + KL Divergence                â”‚
â”‚                                                              â”‚
â”‚   Applications:                                             â”‚
â”‚   - Image generation                                        â”‚
â”‚   - Anomaly detection                                       â”‚
â”‚   - Data compression                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Diffusion Models

```
Forward Process (Add Noise):
xâ‚€ â”€â”€â†’ xâ‚ â”€â”€â†’ xâ‚‚ â”€â”€â†’ ... â”€â”€â†’ xâ‚œ â”€â”€â†’ Pure Noise

Reverse Process (Remove Noise):
Noise â”€â”€â†’ xâ‚œâ‚‹â‚ â”€â”€â†’ ... â”€â”€â†’ xâ‚ â”€â”€â†’ xâ‚€ (Generated Image)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training:                                                    â”‚
â”‚ 1. Take clean image                                         â”‚
â”‚ 2. Add noise at random timestep t                           â”‚
â”‚ 3. Train model to predict the noise                         â”‚
â”‚                                                              â”‚
â”‚ Generation:                                                  â”‚
â”‚ 1. Start with random noise                                  â”‚
â”‚ 2. Iteratively denoise using trained model                  â”‚
â”‚ 3. Obtain final image                                       â”‚
â”‚                                                              â”‚
â”‚ Models: DDPM, Stable Diffusion, DALL-E 2, Midjourney       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### LLMs (Large Language Models)

```
Key Concepts:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pre-training:                                                â”‚
â”‚ - Train on massive text corpus (internet, books, etc.)      â”‚
â”‚ - Learn general language understanding                       â”‚
â”‚ - Self-supervised (next token prediction)                   â”‚
â”‚                                                              â”‚
â”‚ Scaling Laws:                                                â”‚
â”‚ - More parameters = better performance                      â”‚
â”‚ - More data = better performance                            â”‚
â”‚ - More compute = better performance                         â”‚
â”‚                                                              â”‚
â”‚ Emergent Abilities:                                         â”‚
â”‚ - In-context learning                                       â”‚
â”‚ - Chain-of-thought reasoning                                â”‚
â”‚ - Few-shot learning                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Notable LLMs:
- GPT-3/4 (OpenAI)
- Claude (Anthropic)
- PaLM/Gemini (Google)
- LLaMA (Meta)
- Mistral/Mixtral (Mistral AI)
```

### Fine-tuning

```
Types of Fine-tuning:

Full Fine-tuning:
- Update all model parameters
- Requires significant compute
- Risk of catastrophic forgetting

Parameter-Efficient Fine-tuning (PEFT):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LoRA (Low-Rank Adaptation):                                 â”‚
â”‚ - Freeze original weights                                   â”‚
â”‚ - Add trainable low-rank matrices                          â”‚
â”‚ - W' = W + BA where B, A are small matrices               â”‚
â”‚                                                              â”‚
â”‚ Adapter Layers:                                             â”‚
â”‚ - Insert small trainable modules between frozen layers     â”‚
â”‚                                                              â”‚
â”‚ Prefix Tuning:                                              â”‚
â”‚ - Add trainable tokens to input                            â”‚
â”‚                                                              â”‚
â”‚ Prompt Tuning:                                              â”‚
â”‚ - Learn soft prompts in embedding space                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RLHF (Reinforcement Learning from Human Feedback)

```
RLHF Pipeline:

Step 1: Supervised Fine-tuning (SFT)
- Fine-tune base model on high-quality demonstrations

Step 2: Reward Model Training
- Humans rank multiple model outputs
- Train reward model to predict human preferences

Step 3: RL Fine-tuning (PPO)
- Use reward model as reward signal
- Optimize policy to maximize reward
- KL penalty to prevent divergence from SFT model

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  Prompt â”€â”€â†’ [LLM] â”€â”€â†’ Response â”€â”€â†’ [Reward Model] â”€â”€â†’ Score â”‚
â”‚              â†‘                                       â”‚      â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                    Update via PPO                           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Prompt Engineering Techniques

| Technique | Description | Example |
|-----------|-------------|---------|
| **Zero-shot** | Direct task without examples | "Translate to French: Hello" |
| **Few-shot** | Provide examples in prompt | "Q: 2+2=? A: 4, Q: 3+3=? A: 6, Q: 5+5=?" |
| **Chain-of-Thought** | Request step-by-step reasoning | "Think step by step..." |
| **Self-Consistency** | Sample multiple responses, take majority | Generate 5 answers, vote |
| **Tree-of-Thought** | Explore multiple reasoning paths | Branch and evaluate |
| **ReAct** | Reasoning + Acting in loops | Thought â†’ Action â†’ Observation |

---

## ğŸ® Reinforcement Learning

### Core Concepts

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RL Framework                               â”‚
â”‚                                                              â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                    â”‚  Agent  â”‚                              â”‚
â”‚                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                              â”‚
â”‚                         â”‚                                    â”‚
â”‚            Action (aâ‚œ)  â”‚   State (sâ‚œ)                      â”‚
â”‚                    â†“    â”‚    â†‘                              â”‚
â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                â”‚   Environment   â”‚                          â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                         â”‚                                    â”‚
â”‚                    Reward (râ‚œ)                              â”‚
â”‚                                                              â”‚
â”‚  Goal: Learn policy Ï€(a|s) that maximizes cumulative reward â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key Terms:
- State (s): Current situation
- Action (a): What agent can do
- Reward (r): Feedback signal
- Policy (Ï€): Action selection strategy
- Value (V): Expected future reward from state
- Q-value (Q): Expected future reward from state-action pair
```

### Q-Learning

```
Q-Learning Update:
Q(s,a) â† Q(s,a) + Î± Ã— [r + Î³ Ã— max Q(s',a') - Q(s,a)]

where:
Î± = learning rate
Î³ = discount factor (importance of future rewards)
r = immediate reward
s' = next state

Q-Table Example:
         Action 1   Action 2   Action 3
State 1    0.5       0.2        0.8
State 2    0.1       0.9        0.3
State 3    0.7       0.4        0.6

DQN (Deep Q-Network):
- Replace Q-table with neural network
- Experience replay buffer
- Target network for stable learning
```

### Policy Gradients

```
Direct Policy Optimization:

Instead of learning value function, directly learn policy

Policy Gradient Theorem:
âˆ‡J(Î¸) = ğ”¼[âˆ‡log Ï€(a|s;Î¸) Ã— R]

REINFORCE Algorithm:
1. Sample trajectory using current policy
2. Compute returns for each step
3. Update policy: Î¸ â† Î¸ + Î± Ã— âˆ‡log Ï€(a|s;Î¸) Ã— G

Problem: High variance
Solution: Subtract baseline (e.g., value function)
```

### Actor-Critic Methods

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Actor-Critic Architecture                     â”‚
â”‚                                                              â”‚
â”‚     State â”€â”€â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚               â”‚   Actor    â”‚ â”€â”€â†’ Action (policy)            â”‚
â”‚               â”‚  (Policy)  â”‚                                â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                              â”‚
â”‚     State â”€â”€â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚               â”‚   Critic   â”‚ â”€â”€â†’ Value (evaluation)         â”‚
â”‚               â”‚  (Value)   â”‚                                â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                              â”‚
â”‚  Actor: Decides which action to take                        â”‚
â”‚  Critic: Evaluates how good the action was                  â”‚
â”‚  Training: Critic's evaluation guides actor's updates       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Variants:
- A2C: Advantage Actor-Critic
- A3C: Asynchronous A3C
- PPO: Proximal Policy Optimization
- SAC: Soft Actor-Critic
```

---

## âš–ï¸ AI Ethics & Safety

### Bias in AI

```
Types of Bias:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Historical Bias:                                            â”‚
â”‚ - Bias present in training data from society               â”‚
â”‚                                                              â”‚
â”‚ Representation Bias:                                        â”‚
â”‚ - Underrepresentation of certain groups                    â”‚
â”‚                                                              â”‚
â”‚ Measurement Bias:                                           â”‚
â”‚ - Flawed metrics or proxy variables                        â”‚
â”‚                                                              â”‚
â”‚ Aggregation Bias:                                           â”‚
â”‚ - One-size-fits-all models for diverse groups              â”‚
â”‚                                                              â”‚
â”‚ Evaluation Bias:                                            â”‚
â”‚ - Testing on non-representative benchmarks                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Mitigation Strategies:
- Diverse and representative training data
- Bias audits and testing
- Fairness constraints in optimization
- Continuous monitoring in production
```

### Fairness Definitions

| Definition | Description |
|------------|-------------|
| **Demographic Parity** | Equal positive rate across groups |
| **Equalized Odds** | Equal TPR and FPR across groups |
| **Individual Fairness** | Similar individuals get similar predictions |
| **Counterfactual Fairness** | Prediction unchanged if protected attribute changed |

### Explainable AI (XAI)

```
Methods:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LIME (Local Interpretable Model-agnostic Explanations):     â”‚
â”‚ - Create interpretable local approximation                  â”‚
â”‚ - Works for any model                                       â”‚
â”‚                                                              â”‚
â”‚ SHAP (SHapley Additive exPlanations):                       â”‚
â”‚ - Based on game theory (Shapley values)                     â”‚
â”‚ - Feature contribution to prediction                        â”‚
â”‚                                                              â”‚
â”‚ Attention Visualization:                                    â”‚
â”‚ - Show which parts of input model attends to               â”‚
â”‚                                                              â”‚
â”‚ Saliency Maps:                                              â”‚
â”‚ - Gradient-based pixel importance for images               â”‚
â”‚                                                              â”‚
â”‚ Concept Activation Vectors (CAV):                           â”‚
â”‚ - Test sensitivity to human concepts                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### AI Alignment

```
Key Challenges:

1. Specification Problem:
   - Difficulty in precisely defining what we want
   - Goodhart's Law: "When a measure becomes a target, 
     it ceases to be a good measure"

2. Robustness Problem:
   - AI behaving correctly in distribution
   - Failing on edge cases or distribution shift

3. Assurance Problem:
   - How do we verify AI is aligned?
   - Can we trust AI's explanations?

4. Deception Risk:
   - Sufficiently capable AI might deceive evaluators
   - Instrumental convergence concerns

Approaches:
- Constitutional AI
- Debate (AI arguing with itself)
- Recursive reward modeling
- Interpretability research
```

### Responsible AI Practices

```
Framework:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  1. TRANSPARENCY                                            â”‚
â”‚     - Document model cards                                  â”‚
â”‚     - Publish limitations                                   â”‚
â”‚     - Clear usage guidelines                                â”‚
â”‚                                                              â”‚
â”‚  2. ACCOUNTABILITY                                          â”‚
â”‚     - Clear ownership                                       â”‚
â”‚     - Audit trails                                          â”‚
â”‚     - Incident response plans                               â”‚
â”‚                                                              â”‚
â”‚  3. PRIVACY                                                 â”‚
â”‚     - Data minimization                                     â”‚
â”‚     - Consent mechanisms                                    â”‚
â”‚     - Differential privacy                                  â”‚
â”‚                                                              â”‚
â”‚  4. SECURITY                                                â”‚
â”‚     - Adversarial robustness                               â”‚
â”‚     - Access controls                                       â”‚
â”‚     - Regular security audits                              â”‚
â”‚                                                              â”‚
â”‚  5. HUMAN OVERSIGHT                                         â”‚
â”‚     - Human-in-the-loop for critical decisions             â”‚
â”‚     - Appeal mechanisms                                     â”‚
â”‚     - Regular human review                                  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

<div align="center">

## ğŸ“š Continue Learning

| Section | Link |
|---------|------|
| ğŸ’» Code Examples | [Browse Code â†’](../code/README.md) |
| ğŸ”— Resources | [Browse Resources â†’](../resources/README.md) |
| ğŸ“‹ Cheatsheets | [Browse Cheatsheets â†’](../cheatsheets/README.md) |
| ğŸ“– Glossary | [Browse Glossary â†’](../glossary/README.md) |

---

[â† Back to Main](../README.md)

</div>

# ğŸ”„ Transformers & Attention

## Table of Contents
- [Introduction](#introduction)
- [Attention Mechanism](#attention-mechanism)
- [Self-Attention](#self-attention)
- [Multi-Head Attention](#multi-head-attention)
- [Transformer Architecture](#transformer-architecture)
- [Positional Encoding](#positional-encoding)
- [BERT](#bert)
- [GPT](#gpt)
- [Modern Variants](#modern-variants)

---

## Introduction

The Transformer architecture, introduced in "Attention Is All You Need" (2017), revolutionized NLP and has become the foundation for modern AI systems.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   EVOLUTION OF SEQUENCE MODELS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   2014        2015         2017          2018         2020+            â”‚
â”‚                                                                         â”‚
â”‚   RNN    â†’   LSTM    â†’  Attention  â†’  BERT/GPT  â†’   GPT-3/4           â”‚
â”‚    â”‚          â”‚            â”‚             â”‚            â”‚                â”‚
â”‚  Simple    Better at     No more       Pre-train   Emergent           â”‚
â”‚ Sequences  Long-range    Sequential    + Fine-tune Capabilities       â”‚
â”‚            Dependencies  Processing                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Attention Mechanism

### The Intuition

Attention allows models to focus on relevant parts of the input when producing each part of the output.

```
    "The cat sat on the mat because it was tired"
                                    â†‘
                                   "it"
                                    â”‚
              attention weights     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  0.1   0.7   0.05  0.02 0.02  0.05  0.01  0.03  0.02  â”‚
    â”‚   â”‚     â”‚     â”‚     â”‚    â”‚     â”‚     â”‚     â”‚     â”‚    â”‚
    â”‚  The   cat   sat   on  the    mat because  it   was  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    "it" attends most strongly to "cat" (0.7)
```

### Query, Key, Value

The fundamental building blocks:

- **Query (Q)**: What I'm looking for
- **Key (K)**: What I have to offer  
- **Value (V)**: What I actually give you

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ATTENTION AS DATABASE LOOKUP                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Query: "What is the capital of France?"                           â”‚
â”‚                                                                      â”‚
â”‚   Key-Value pairs in "database":                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚           KEY               â”‚           VALUE              â”‚    â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚   â”‚ "capital of France"         â”‚ "Paris"              â† MATCH â”‚    â”‚
â”‚   â”‚ "population of France"      â”‚ "67 million"                 â”‚    â”‚
â”‚   â”‚ "capital of Germany"        â”‚ "Berlin"                     â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                      â”‚
â”‚   Attention = softmax(similarity(Q, K)) Ã— V                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Self-Attention

### Scaled Dot-Product Attention

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Where $d_k$ is the dimension of the keys (scaling prevents softmax from having extremely small gradients).

### Step-by-Step Computation

```
Input: "The cat sat"
       [xâ‚] [xâ‚‚] [xâ‚ƒ]

Step 1: Create Q, K, V matrices
        Q = X Ã— W_Q    (queries)
        K = X Ã— W_K    (keys)
        V = X Ã— W_V    (values)

Step 2: Compute attention scores
                    K^T
              [kâ‚ kâ‚‚ kâ‚ƒ]
        Q  â”Œ             â”
       [qâ‚]â”‚ sâ‚â‚ sâ‚â‚‚ sâ‚â‚ƒâ”‚
       [qâ‚‚]â”‚ sâ‚‚â‚ sâ‚‚â‚‚ sâ‚‚â‚ƒâ”‚  = QK^T / âˆšd_k
       [qâ‚ƒ]â”‚ sâ‚ƒâ‚ sâ‚ƒâ‚‚ sâ‚ƒâ‚ƒâ”‚
           â””             â”˜

Step 3: Apply softmax (row-wise)
        â”Œ                 â”
        â”‚ Î±â‚â‚ Î±â‚â‚‚ Î±â‚â‚ƒ    â”‚  where Î£â±¼ Î±áµ¢â±¼ = 1
        â”‚ Î±â‚‚â‚ Î±â‚‚â‚‚ Î±â‚‚â‚ƒ    â”‚
        â”‚ Î±â‚ƒâ‚ Î±â‚ƒâ‚‚ Î±â‚ƒâ‚ƒ    â”‚
        â””                 â”˜

Step 4: Weighted sum of values
        Output = Attention_weights Ã— V
```

### Why Self-Attention?

| Aspect | RNN | Self-Attention |
|--------|-----|----------------|
| **Sequential** | Yes, must process in order | No, parallel processing |
| **Max Path Length** | O(n) | O(1) |
| **Computation** | O(n) | O(nÂ²) |
| **Long-range Dependencies** | Hard (vanishing gradients) | Easy (direct connections) |

---

## Multi-Head Attention

Instead of one attention function, run multiple in parallel:

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

Where each head:
$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MULTI-HEAD ATTENTION                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚              Input (Q, K, V)                                           â”‚
â”‚                    â”‚                                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚    â”‚       â”‚       â”‚       â”‚       â”‚                                   â”‚
â”‚    â–¼       â–¼       â–¼       â–¼       â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”                                â”‚
â”‚  â”‚ H1â”‚   â”‚ H2â”‚   â”‚ H3â”‚   â”‚...â”‚   â”‚ Hhâ”‚   h = 8 typically             â”‚
â”‚  â””â”€â”¬â”€â”˜   â””â”€â”¬â”€â”˜   â””â”€â”¬â”€â”˜   â””â”€â”¬â”€â”˜   â””â”€â”¬â”€â”˜                                â”‚
â”‚    â”‚       â”‚       â”‚       â”‚       â”‚                                   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                    â”‚                                                    â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                              â”‚
â”‚              â”‚  Concat   â”‚                                              â”‚
â”‚              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚                    â”‚                                                    â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                              â”‚
â”‚              â”‚  Linear   â”‚                                              â”‚
â”‚              â”‚   (W^O)   â”‚                                              â”‚
â”‚              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚                    â”‚                                                    â”‚
â”‚                 Output                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Each head can learn different types of relationships:
- Head 1: Syntactic relationships
- Head 2: Semantic relationships  
- Head 3: Positional patterns
- etc.
```

---

## Transformer Architecture

### The Full Picture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRANSFORMER ARCHITECTURE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚        ENCODER (x6)                        DECODER (x6)                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚                     â”‚            â”‚                     â”‚           â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚           â”‚
â”‚   â”‚  â”‚ Feed Forward  â”‚  â”‚            â”‚  â”‚ Feed Forward  â”‚  â”‚           â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚           â”‚
â”‚   â”‚          â”‚ Add&Norm â”‚            â”‚          â”‚ Add&Norm â”‚           â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”  â”‚           â”‚
â”‚   â”‚  â”‚  Multi-Head   â”‚  â”‚   â”€â”€â”€â”€â”€â–º   â”‚  â”‚  Multi-Head   â”‚  â”‚           â”‚
â”‚   â”‚  â”‚   Attention   â”‚  â”‚   Context  â”‚  â”‚   Attention   â”‚  â”‚  Cross    â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚  â”‚   (Cross)     â”‚  â”‚  Attn     â”‚
â”‚   â”‚          â”‚ Add&Norm â”‚            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚           â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚          â”‚ Add&Norm â”‚           â”‚
â”‚   â”‚  â”‚  Multi-Head   â”‚  â”‚            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”  â”‚           â”‚
â”‚   â”‚  â”‚ Self-Attentionâ”‚  â”‚            â”‚  â”‚  Masked MH    â”‚  â”‚           â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚  â”‚ Self-Attentionâ”‚  â”‚           â”‚
â”‚   â”‚          â”‚ Add&Norm â”‚            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚          â”‚ Add&Norm â”‚           â”‚
â”‚              â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚      â”‚   Positional  â”‚                   â”‚   Positional  â”‚             â”‚
â”‚      â”‚   Encoding    â”‚                   â”‚   Encoding    â”‚             â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚              â”‚                                   â”‚                      â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚      â”‚   Embedding   â”‚                   â”‚   Embedding   â”‚             â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚              â”‚                                   â”‚                      â”‚
â”‚           Input                              Output                     â”‚
â”‚          Tokens                          Tokens (shifted)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

#### 1. Add & Norm (Residual Connection + Layer Norm)
$$\text{Output} = \text{LayerNorm}(x + \text{Sublayer}(x))$$

#### 2. Feed-Forward Network
$$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$$

Two linear transformations with ReLU in between:
- $d_{model} = 512$
- $d_{ff} = 2048$ (4x expansion)

---

## Positional Encoding

Since transformers have no inherent notion of position, we inject positional information:

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    POSITIONAL ENCODING                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Position 0:  [sin(0/1),   cos(0/1),   sin(0/100), cos(0/100), ...]  â”‚
â”‚   Position 1:  [sin(1/1),   cos(1/1),   sin(1/100), cos(1/100), ...]  â”‚
â”‚   Position 2:  [sin(2/1),   cos(2/1),   sin(2/100), cos(2/100), ...]  â”‚
â”‚   ...                                                                   â”‚
â”‚                                                                         â”‚
â”‚   High frequency (small i): Capture fine-grained position              â”‚
â”‚   Low frequency (large i): Capture coarse position                     â”‚
â”‚                                                                         â”‚
â”‚   Final Input = Token Embedding + Positional Encoding                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## BERT

**B**idirectional **E**ncoder **R**epresentations from **T**ransformers

### Architecture

- Encoder-only transformer
- Pre-training tasks:
  1. **Masked Language Modeling (MLM)**: Predict [MASK] tokens
  2. **Next Sentence Prediction (NSP)**: Is sentence B after A?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         BERT OVERVIEW                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   PRE-TRAINING                              FINE-TUNING                â”‚
â”‚                                                                         â”‚
â”‚   "The [MASK] sat on mat"                   Task-specific head         â”‚
â”‚         â†“                                          â†‘                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚    BERT     â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚    BERT     â”‚           â”‚
â”‚   â”‚   Encoder   â”‚  (Transfer weights)       â”‚   Encoder   â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚         â†“                                          â†“                   â”‚
â”‚   Predict "cat"                             Classification/QA/NER      â”‚
â”‚                                                                         â”‚
â”‚   BERT-Base: 12 layers, 768 hidden, 12 heads, 110M params             â”‚
â”‚   BERT-Large: 24 layers, 1024 hidden, 16 heads, 340M params           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Input Format

```
[CLS] Sentence A [SEP] Sentence B [SEP]
  â”‚       â”‚        â”‚        â”‚       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
          Token Embeddings
                +
          Segment Embeddings (A vs B)
                +
          Position Embeddings
```

---

## GPT

**G**enerative **P**re-trained **T**ransformer

### Architecture

- Decoder-only transformer
- Autoregressive: Predict next token
- Causal attention mask: Can only attend to previous tokens

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         GPT OVERVIEW                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   AUTOREGRESSIVE GENERATION                                            â”‚
â”‚                                                                         â”‚
â”‚   Input:  "The cat sat"                                                â”‚
â”‚                â†“                                                        â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚           â”‚     GPT     â”‚                                              â”‚
â”‚           â”‚   Decoder   â”‚                                              â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚                â†“                                                        â”‚
â”‚   Output: "on" (next token prediction)                                 â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                    CAUSAL ATTENTION MASK                       â”‚   â”‚
â”‚   â”‚                                                                â”‚   â”‚
â”‚   â”‚            The   cat   sat   on                                â”‚   â”‚
â”‚   â”‚     The  [  1     0     0    0  ]                              â”‚   â”‚
â”‚   â”‚     cat  [  1     1     0    0  ]    Can only attend to       â”‚   â”‚
â”‚   â”‚     sat  [  1     1     1    0  ]    previous tokens          â”‚   â”‚
â”‚   â”‚     on   [  1     1     1    1  ]    (including self)         â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚   GPT-2: 1.5B params | GPT-3: 175B params | GPT-4: ~1T params?        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Training Objective

Next token prediction (language modeling):

$$P(w_1, w_2, ..., w_n) = \prod_{i=1}^{n} P(w_i | w_1, ..., w_{i-1})$$

Loss:
$$L = -\sum_{i} \log P(w_i | w_1, ..., w_{i-1})$$

---

## Modern Variants

### Comparison Table

| Model | Type | Key Innovation | Size |
|-------|------|----------------|------|
| **BERT** | Encoder | Bidirectional, MLM | 110M-340M |
| **GPT-2/3/4** | Decoder | Scale, emergent abilities | 1.5B-1T+ |
| **T5** | Enc-Dec | Text-to-text framework | 220M-11B |
| **LLaMA** | Decoder | Open weights, efficiency | 7B-70B |
| **Claude** | Decoder | Constitutional AI | - |
| **Mistral** | Decoder | Sliding window attention | 7B |

### Key Innovations

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRANSFORMER EVOLUTION                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   2017: Original Transformer                                        â”‚
â”‚         â””â”€â”€ 2018: BERT, GPT                                         â”‚
â”‚                   â””â”€â”€ 2019: GPT-2, RoBERTa, ALBERT                  â”‚
â”‚                            â””â”€â”€ 2020: GPT-3, T5                      â”‚
â”‚                                     â””â”€â”€ 2022: ChatGPT, PaLM        â”‚
â”‚                                              â””â”€â”€ 2023: GPT-4,       â”‚
â”‚                                                       LLaMA,        â”‚
â”‚                                                       Claude        â”‚
â”‚                                                       â””â”€â”€ 2024+:    â”‚
â”‚                                                           Multimodalâ”‚
â”‚                                                           Efficient â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Efficient Transformers

- **Flash Attention**: Memory-efficient attention computation
- **Sparse Attention**: Only attend to subset of tokens
- **Linear Attention**: O(n) instead of O(nÂ²)
- **Mixture of Experts (MoE)**: Conditional computation

---

## Resources

- ğŸ“„ **Paper**: "Attention Is All You Need" (Vaswani et al., 2017)
- ğŸ“„ **Paper**: "BERT: Pre-training of Deep Bidirectional Transformers" (Devlin et al., 2018)
- ğŸ“„ **Paper**: "Language Models are Few-Shot Learners" (GPT-3, Brown et al., 2020)
- ğŸ“ **Course**: Stanford CS224N - NLP with Deep Learning
- ğŸ“ **Blog**: "The Illustrated Transformer" by Jay Alammar

---

ğŸŒ [Back to Notes](README.md) | ğŸ”— [Visit jgcks.com](https://www.jgcks.com)
